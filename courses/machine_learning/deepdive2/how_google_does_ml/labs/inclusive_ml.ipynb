{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inclusive ML - Understanding Bias\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "In this lab, you use a Juypter Notebook to:\n",
    "- Invoke the What-if Tool against a deployed Model\n",
    "- Explore attributes of the dataset\n",
    "- Examine aspects of bias in model results\n",
    "- Evaluate how the What-if Tool provides suggestions to remediate bias\n",
    "\n",
    "---\n",
    "## Introduction \n",
    "\n",
    "This notebook shows use of the [What-If Tool](https://pair-code.github.io/what-if-tool) inside of a Jupyter notebook.  The What-If Tool, among many other things, allows us to explore the impacts of Fairness in model design and deployment.\n",
    "\n",
    "The notebook invokes a deployed XGBoost classifier model on the [UCI census dataset](https://archive.ics.uci.edu/ml/datasets/census+income) which predicts whether a person earns more than $50K based on their census information.\n",
    "\n",
    "You will then visualize the results of the trained classifier on test data using the What-If Tool.  \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the notebook environment\n",
    "\n",
    "First you must perform a few environment and project configuration steps.  \n",
    "\n",
    "__VERY IMPORTANT__:  In the cell below you must replace the text 'QWIKLABSPROJECT' with your Qwiklabs Project Name as provided during the setup of your environment. Please leave any surrounding single quotes in place.\n",
    "\n",
    "These steps may take 8 - 10 minutes, please wait until you see the following response before proceeding:  \"__Creating version (this might take a few minutes)......done.__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab Setup\n",
    "# TODO Replace the name QWIKLABSPROJECT with your Qwiklabs Project Name, leave single quotes in place\n",
    "GCP_PROJECT = 'QWIKLABSPROJECT' #TODO\n",
    "!gsutil mb -p $GCP_PROJECT gs://$GCP_PROJECT\n",
    "MODEL_BUCKET = 'gs://QWIKLABSPROJECT' #TODO\n",
    "!gsutil cp gs://cloud-training-demos/mlfairness/model.bst $MODEL_BUCKET\n",
    "!gcloud config set project $GCP_PROJECT\n",
    "!gcloud ai-platform models create model\n",
    "!gcloud ai-platform versions create 'v1' \\\n",
    "--model=model \\\n",
    "--framework='XGBOOST' \\\n",
    "--runtime-version=1.14 \\\n",
    "--origin=$MODEL_BUCKET \\\n",
    "--python-version=3.5 \\\n",
    "--project=$GCP_PROJECT\n",
    "\n",
    "print(\"Cell Successfully Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Next import the libraries needed for the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import witwidget\n",
    "from witwidget.notebook.visualization import WitWidget, WitConfigBuilder\n",
    "print(\"Cell Successfully Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Finally download the data and arrays needed to use the What-if Tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp gs://cloud-training-demos/mlfairness/income.pkl .\n",
    "!gsutil cp gs://cloud-training-demos/mlfairness/x_test.npy .\n",
    "!gsutil cp gs://cloud-training-demos/mlfairness/y_test.npy .\n",
    "features = pd.read_pickle('income.pkl')\n",
    "x_test = np.load('x_test.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "print(\"Cell Successfully Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now take a quick look at the data.  The ML model type used for this analysis is XGBoost.  XGBoost is a machine learning framework that uses decision trees and gradient boosting to build predictive models. It works by ensembling multiple decision trees together based on the score associated with different leaf nodes in a tree. \n",
    "\n",
    "XGBoost requires all values to be numeric so the orginial dataset was modified slightly.  The biggest change made was to assign a numeric value to Sex.  The originial dataset only had the values \"Female\" and \"Male\" for Sex.  The decision was made to assign the value \"1\" to Female and \"2\" to Male. As part of the data prepartion effort the Pandas function \"get_dummies\" was used to convert the remaining domain values into numerical equiavlents.  For instance the \"Education\" column was turned into several sub-columns named after the value in the column.  For instance the \"Education_HS-grad\" has a value of \"1\" for when that was the orginial categorical value and a value of \"0\" for other cateogries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "To connect the What-if Tool to an AI Platform model, you need to pass it a subset of your test examples.  The commannd below will create a Numpy array of 2000 from our test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the features and labels into one array for the What-if Tool\n",
    "num_wit_examples = 2000\n",
    "test_examples = np.hstack((x_test[:num_wit_examples],y_test[:num_wit_examples].reshape(-1,1)))\n",
    "print(\"Cell Successfully Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Instantiating the What-if Tool is as simple as creating a WitConfigBuilder object and passing it the AI Platform model desired to be analyzed.\n",
    "\n",
    "The optional \"adjust_prediction\" parameter is used because the What-if Tool expects a list of scores for each class in our model (in this case 2). Since the model only returns a single value from 0 to 1, it must be transformed to the correct format in this function.   Lastly, the name 'income_prediction' is used as the ground truth label.\n",
    "\n",
    "It may take 1 to 2 minutes for the What-if Tool to load and render the visualization palette, please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_prediction(pred):\n",
    "  return [1 - pred, pred]\n",
    "\n",
    "config_builder = (WitConfigBuilder(test_examples.tolist(), features.columns.tolist() + ['income_prediction'])\n",
    "  .set_ai_platform_model('QWIKLABSPROJECT', 'model', 'v1', adjust_prediction=adjust_prediction)\n",
    "  .set_target_feature('income_prediction')\n",
    "  .set_label_vocab(['low', 'high']))\n",
    "WitWidget(config_builder, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## In the following steps we will use the What-if Tool to examine our model \n",
    "\n",
    "Since the What-If Tool is embedded in the notebook you must use care when scrolling.  The What-if Tool has its own internal scrolling windows so you may need to reposition the frame window to reach the desired location.  To do this you must ensure you are at the far edges of the cell to scroll up and down as noted by the red oblong markers in the image below.\n",
    "\n",
    "Also, it might be helpful to widen the display frame in the notebook.  You can do this by dragging the vertical grey bar to the left (the place to click and hold is noted by the red arrows).\n",
    "\n",
    "![alt_text](https://storage.cloud.google.com/cloud-training-demos/mlfairness/images/WIT_25.png \"WIT UI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The scenario for consideration is that this model, which is used to predict income levels, will be used in a loan approval process.   Your task is to determine its fitness for such a use case from a fairness perspective.\n",
    "\n",
    "Your first action will be to examine data and its disribution along dimensions that are relevant to loan scoring.   The intial presentation in the tool shows all datapoints.  Blue dots are those individuals predicted as having incomes above 50k.  Red dots are those predicted as having incomes below 50k.\n",
    "\n",
    "On the \"Datapoint Editor\" tab, under \"Binning | X-Axis\" select \"Education_Masters\":\n",
    "\n",
    "![alt_text](https://storage.cloud.google.com/cloud-training-demos/mlfairness/images/BinX_Menu_Choice.png \"BinX Menu\")\n",
    "\n",
    "The results show that a majority of Masters degree holders, domain value \"1\" (in red above the visualization), have incomes above 50k.\n",
    "\n",
    "![alt_text](https://storage.cloud.google.com/cloud-training-demos/mlfairness/images/BinX_Results.png \"BinX Results\")\n",
    "\n",
    "Return to the \"Binning | X-Axis\" selector and choose \"None\" to reset the visualization.\n",
    "\n",
    "---\n",
    "\n",
    "Next navigate to the Features tab, here you can see the exact distribution of values for every feature in the dataset.  If you type \"sex\" into the filter box, you will see that of the 2,000 test datapoints, 670 from Women and 1,330 are from Men (as mentioned earlier the value \"1\" was assigned to Females and \"2\" was assigned to Males). The dataset reflects an imbalance between Females and Males with nearly double the number of cases that are Male. Women seem under-represented in this dataset.\n",
    "\n",
    "![alt_text](https://storage.cloud.google.com/cloud-training-demos/mlfairness/images/Features.png \"Features\")\n",
    "\n",
    "On the \"Performance + Fairness\" tab, you can set an input feature (or set of features) by which to slice the data. This will allow you to evaluate the fairness of specific groups.  Income Prediction (corresponding to over or under 50k) has already been selected as the \"Ground Truth Feature\". On the \"Slice by\" selector, scroll to find and choose \"Sex\".\n",
    "\n",
    "![alt_text](https://storage.cloud.google.com/cloud-training-demos/mlfairness/images/PerfTab.png \"PerfTab\")\n",
    "\n",
    "This selection allows you to see the breakdown of model performance on female datapoints versus male datapoints.  Even before you drill into the details you can see that the model has a higher false accuracy value for females than males. Drilling down on each value (by clicking the arrow beside the domain value) you see that the model predicts high income for females much less than it does for males:  3.4% of the time for females vs 18.9% of the time for males.\n",
    "\n",
    "![alt_text](https://storage.cloud.google.com/cloud-training-demos/mlfairness/images/MaleDetails.png \"Male Details\")\n",
    "\n",
    "![alt_text](https://storage.cloud.google.com/cloud-training-demos/mlfairness/images/FemaleDetails.png \"Female Details\")\n",
    "\n",
    "In the use-case scenario, the plan is to use this simple income classifier to approve or reject loan applications (not a realistic example but it illustrates the effect of bias in ML usage). In this case, 18.9% of men from the test dataset have their loans approved but only 3.4% of women have their loans approved. If you wanted to ensure than men and women get their loans approved the same percentage of the time, that is a fairness concept called \"demographic parity\". An XGBoost model intially defaults to a 0.50 threshold, which is what appears upon initial examination.  One way to achieve demographic parity would be to have different classification thresholds for females and males in our model. You'll notice there is a button on the tool labeled \"demographic parity\". When you press this button, the tool will take the cost ratio into account, and come up with ideal separate thresholds for men and women that will achieve demographic parity over the test dataset.\n",
    "\n",
    "On the \"Performance + Fairness\" tab, select \"Demographic Parity\" to see the results.\n",
    "\n",
    "![alt_text](https://storage.cloud.google.com/cloud-training-demos/mlfairness/images/DemoParityButton_Resize.png \"Demographic Parity Button\")\n",
    "\n",
    "By drilling down on each domain value you can see the automatic adjustments.  In this case, demographic parity can be found with both groups getting loans approved/predicting a high income ~17.7% of the time.  This occurs when the female threshold is set to 0.19 and the male threshold is set to 0.54. Because of the vast difference in the properties of the female and male training data in this 1994 census dataset, you need quite different thresholds to achieve demographic parity. With the high male threshold you may notice there are many more false negatives than before, and with the low female threshold there are many more false positives than before.  To reset the Peformance and Fairness Tab simply choose another domain value in the \"Slice by\" selector.\n",
    "\n",
    "![alt_text](https://storage.cloud.google.com/cloud-training-demos/mlfairness/images/MaleDemoParitySlider.png \"Male Demo Parity\")\n",
    "\n",
    "![alt_text](https://storage.cloud.google.com/cloud-training-demos/mlfairness/images/FemaleDemoParitySlider.png \"Female Demo Parity\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The use of these features can help shed light on subsets of your data on which your classifier is performing very differently. Understanding biases in your datasets and data slices on which your model has disparate performance are very important parts of analyzing a model for fairness. There are many approaches to improving fairness, including augmenting training data, building fairness-related loss functions into your model training procedure, and post-training inference adjustments like those seen in WIT. The WIT provides a great interface for furthering ML fairness learning, but of course there is no silver bullet to improving ML fairness.\n",
    "\n",
    "Feel free to explore the What-if Tool and find additional insights.\n",
    "\n",
    "This is the end of the lab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
